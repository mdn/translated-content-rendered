---
title: Utilizar a API de Áudio da Web
slug: Web/API/Web_Audio_API/Utilizar_api_audio_web
tags:
  - API
  - API de Áudio da Web
  - Guía
  - Referencia
  - Utilização
  - básicos
translation_of: Web/API/Web_Audio_API/Using_Web_Audio_API
---
<div><section class="Quick_links" id="Quick_Links"><ol><li><strong><a href="/pt-PT/docs/Web/API/Web_Audio_API">Web Audio API</a></strong></li><li class="toggle"><details open><summary>Guides</summary><ol><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Advanced_techniques">Advanced techniques: creating sound, sequencing, timing, scheduling</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Advanced_techniques$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode">Controlling multiple parameters with ConstantSourceNode</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext">Porting webkitAudioContext code to standards based AudioContext</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Simple_synth">Simple synth keyboard</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Simple_synth$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Tools">Tools for analyzing Web Audio usage</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Tools$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Using_IIR_filters">Using IIR filters</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Using_IIR_filters$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Utilizar_api_audio_web">Utilizar a API de Áudio da Web</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Best_practices">Web Audio API best practices</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Best_practices$translate">[Translate]</a></li><li><a href="/pt-PT/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics">Web audio spatialization basics</a> <a style="opacity: 0.5;" href="/pt-PT/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics$translate">[Translate]</a></li></ol></details></li><li class="toggle"><details open><summary>Interfaces</summary><ol><li><a href="/pt-PT/docs/Web/API/AnalyserNode"><code>AnalyserNode</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioBuffer"><code>AudioBuffer</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioBufferSourceNode"><code>AudioBufferSourceNode</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioContext"><code>AudioContext</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioContextOptions"><code>AudioContextOptions</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioDestinationNode"><code>AudioDestinationNode</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioListener"><code>AudioListener</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioNode"><code>AudioNode</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioNodeOptions"><code>AudioNodeOptions</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioParam"><code>AudioParam</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioProcessingEvent"><code>AudioProcessingEvent</code></a></li><li><a href="/pt-PT/docs/Web/API/AudioScheduledSourceNode"><code>AudioScheduledSourceNode</code></a></li><li><a href="/pt-PT/docs/Web/API/BaseAudioContext"><code>BaseAudioContext</code></a></li><li><a href="/pt-PT/docs/Web/API/BiquadFilterNode"><code>BiquadFilterNode</code></a></li><li><a href="/pt-PT/docs/Web/API/ChannelMergerNode"><code>ChannelMergerNode</code></a></li><li><a href="/pt-PT/docs/Web/API/ChannelSplitterNode"><code>ChannelSplitterNode</code></a></li><li><a href="/pt-PT/docs/Web/API/ConstantSourceNode"><code>ConstantSourceNode</code></a></li><li><a href="/pt-PT/docs/Web/API/ConvolverNode"><code>ConvolverNode</code></a></li><li><a href="/pt-PT/docs/Web/API/DelayNode"><code>DelayNode</code></a></li><li><a href="/pt-PT/docs/Web/API/DynamicsCompressorNode"><code>DynamicsCompressorNode</code></a></li><li><a href="/pt-PT/docs/Web/API/GainNode"><code>GainNode</code></a></li><li><a href="/pt-PT/docs/Web/API/IIRFilterNode"><code>IIRFilterNode</code></a></li><li><a href="/pt-PT/docs/Web/API/MediaElementAudioSourceNode"><code>MediaElementAudioSourceNode</code></a></li><li><a href="/pt-PT/docs/Web/API/MediaStreamAudioDestinationNode"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href="/pt-PT/docs/Web/API/MediaStreamAudioSourceNode"><code>MediaStreamAudioSourceNode</code></a></li><li><a href="/pt-PT/docs/Web/API/OfflineAudioCompletionEvent"><code>OfflineAudioCompletionEvent</code></a></li><li><a href="/pt-PT/docs/Web/API/OfflineAudioContext"><code>OfflineAudioContext</code></a></li><li><a href="/pt-PT/docs/Web/API/OscillatorNode"><code>OscillatorNode</code></a></li><li><a href="/pt-PT/docs/Web/API/PannerNode"><code>PannerNode</code></a></li><li><a href="/pt-PT/docs/Web/API/PeriodicWave"><code>PeriodicWave</code></a></li><li><a href="/pt-PT/docs/Web/API/WaveShaperNode"><code>WaveShaperNode</code></a></li><li><a href="/pt-PT/docs/Web/API/StereoPannerNode"><code>StereoPannerNode</code></a></li></ol></details></li><li class="toggle"><details open><summary>Events</summary><ol><li><a href="/pt-PT/docs/Web/Events/statechange"><code>statechange</code></a></li><li><a href="/pt-PT/docs/Web/Events/complete"><code>complete</code></a></li><li><a href="/pt-PT/docs/Web/Events/ended"><code>ended</code></a></li><li><a href="/pt-PT/docs/Web/Events/message"><code>message</code></a></li><li><a href="/pt-PT/docs/Web/Events/loaded"><code>loaded</code></a></li><li><a href="/pt-PT/docs/Web/Events/audioprocess"><code>audioprocess</code></a></li><li><a href="/pt-PT/docs/Web/Events/nodecreate"><code>nodecreate</code></a></li></ol></details></li></ol></section></div>

<div class="summary">
<p>Vamos ver como começar a utilizar a API de Áudio da Web. Nós iremos ver resumidamente alguns conceitos, e depois estudar um exemplo simples de caixa de som que nos permite carregar uma faixa de áudio, reproduzi-la e pausá-la, e alterar o seu volume e <em>panning</em> estéreo<span class="seoSummary">.</span></p>
</div>

<div>
<p>The Web Audio API does not replace the <a href="/en-US/docs/Web/HTML/Element/audio">&lt;audio&gt;</a> media element, but rather complements it, just like <a href="/en-US/docs/Web/HTML/Element/canvas">&lt;canvas&gt;</a> coexists alongside the <a href="/en-US/docs/Web/HTML/Element/Img">&lt;img&gt;</a> element. Your use case will determine what tools you use to implement audio. If you simply want to control playback of an audio track, the &lt;audio&gt; media element provides a better, quicker solution than the Web Audio API. If you want to carry out more complex audio processing, as well as playback, the Web Audio API provides much more power and control.</p>

<p>A powerful feature of the Web Audio API is that it does not have a strict &quot;sound call limitation&quot;. For example, there is no ceiling of 32 or 64 sound calls at one time. Some processors may be capable of playing more than 1,000 simultaneous sounds without stuttering.</p>
</div>

<h2 id="Código_exemplo">Código exemplo</h2>

<p>A nossa caixa de música parece-se com isto:</p>

<p><img src="https://mdn.mozillademos.org/files/16197/boombox.png" alt="A boombox with play, pan, and volume controls" style="border-style: solid; border-width: 1px; height: 646px; width: 1200px;"></p>

<p>Note the retro cassette deck with a play button, and vol and pan sliders to allow you to alter the volume and stereo panning. We could make this a lot more complex, but this is ideal for simple learning at this stage.</p>

<p><a href="https://codepen.io/Rumyra/pen/qyMzqN/">Check out the final demo here on Codepen</a>, or see the <a href="https://github.com/mdn/webaudio-examples/tree/master/audio-basics">source code on GitHub</a>.</p>

<h2 id="Suporte_para_navegador">Suporte para navegador</h2>

<p>Modern browsers have good support for most features of the Web Audio API. There are a lot of features of the API, so for more exact information, you&apos;ll have to check the browser compatibility tables at the bottom of each reference page.</p>

<h2 id="Gráficos_de_áudio">Gráficos de áudio</h2>

<p>Everything within the Web Audio API is based around the concept of an audio graph, which is made up of nodes.</p>

<p>The Web Audio API handles audio operations inside an <strong>audio context</strong>, and has been designed to allow <strong>modular routing</strong>. Basic audio operations are performed with <strong>audio nodes</strong>, which are linked together to form an <strong>audio routing graph</strong>. You have input nodes, which are the source of the sounds you are manipulating, modification nodes that change those sounds as desired, and output nodes (destinations), which allow you to save or hear those sounds.</p>

<p>Several audio sources with different channel layouts are supported, even within a single context. Because of this modular design, you can create complex audio functions with dynamic effects.</p>

<h2 id="Contxeto_de_Áudio">Contxeto de Áudio</h2>

<p>To be able to do anything with the Web Audio API, we need to create an instance of the audio context. This then gives us access to all the features and functionality of the API.</p>

<pre class="brush: js">// for legacy browsers
const AudioContext = window.AudioContext || window.webkitAudioContext;

const audioCtx = new AudioContext();
</pre>

<p>So what&apos;s going on when we do this? A <a href="/pt-PT/docs/Web/API/BaseAudioContext" title="The documentation about this has not yet been written; please consider contributing!"><code>BaseAudioContext</code></a> is created for us automatically and extended to an online audio context. We&apos;ll want this because we&apos;re looking to play live sound.</p>

<div class="note notecard">
<p><strong>Note</strong>: If you just want to process audio data, for instance, buffer and stream it but not play it, you might want to look into creating an <a href="/pt-PT/docs/Web/API/OfflineAudioContext" title="The documentation about this has not yet been written; please consider contributing!"><code>OfflineAudioContext</code></a>.</p>
</div>

<h2 id="Carregar_som">Carregar som</h2>

<p>Now, the audio context we&apos;ve created needs some sound to play through it. There are a few ways to do this with the API. Let&apos;s begin with a simple method — as we have a boombox, we most likely want to play a full song track. Also, for accessibility, it&apos;s nice to expose that track in the DOM. We&apos;ll expose the song on the page using an <a href="/pt-PT/docs/Web/HTML/Element/audio" title="The documentation about this has not yet been written; please consider contributing!"><code>&lt;audio&gt;</code></a> element.</p>

<pre class="brush: html">&lt;audio src=&quot;myCoolTrack.mp3&quot; type=&quot;audio/mpeg&quot;&gt;&lt;/audio&gt;
</pre>

<div class="note notecard">
<p><strong>Nota</strong>: If the sound file you&apos;re loading is held on a different domain you will need to use the <code>crossorigin</code> attribute; see <a href="/en-US/docs/Web/HTTP/CORS">Cross Origin Resource Sharing (CORS)</a>  for more information.</p>
</div>

<p>To use all the nice things we get with the Web Audio API, we need to grab the source from this element and <em>pipe</em> it into the context we have created. Lucky for us there&apos;s a method that allows us to do just that — <a href="/pt-PT/docs/Web/API/AudioContext/createMediaElementSource" title="The documentation about this has not yet been written; please consider contributing!"><code>AudioContext.createMediaElementSource</code></a>:</p>

<pre class="brush: js">// get the audio element
const audioElement = document.querySelector(&apos;audio&apos;);

// pass it into the audio context
const track = audioCtx.createMediaElementSource(audioElement);
</pre>

<div class="note notecard">
<p><strong>Nota</strong>: The <code>&lt;audio&gt;</code> element above is represented in the DOM by an object of type <a href="/pt-PT/docs/Web/API/HTMLMediaElement" title="The documentation about this has not yet been written; please consider contributing!"><code>HTMLMediaElement</code></a>, which comes with its own set of functionality. All of this has stayed intact; we are merely allowing the sound to be available to the Web Audio API.</p>
</div>

<h2 id="Controlar_o_som">Controlar o som</h2>

<p>When playing sound on the web, it&apos;s important to allow the user to control it. Depending on the use case, there&apos;s a myriad of options, but we&apos;ll provide functionality to play/pause the sound, alter the track&apos;s volume, and pan it from left to right.</p>

<div class="note notecard">
<p><strong>Nota</strong>: We need to take into account the new autoplay policy that modern browsers have, which calls for a user gesture before media can play (see Chrome&apos;s <a href="https://developers.google.com/web/updates/2017/09/autoplay-policy-changes">Autoplay Policy Changes</a>, for example). This has been implemented because autoplaying media is really bad for many reasons — it is annoying and intrusive at the very least, and also causes accessibility problems. This is accounted for by our play/pause button.</p>
</div>

<p>Let&apos;s take a look at our play and pause functionality to start with. We have a play button that changes to a pause button when the track is playing:</p>

<pre class="brush: html">&lt;button data-playing=&quot;false&quot; role=&quot;switch&quot; aria-checked=&quot;false&quot;&gt;
    &lt;span&gt;Play/Pause&lt;/span&gt;
&lt;/button&gt;
</pre>

<p>Before we can play our track we need to connect our audio graph from the audio source/input node to the destination.</p>

<p>We&apos;ve already created an input node by passing our audio element into the API. For the most part, you don&apos;t need to create an output node, you can just connect your other nodes to <a href="/pt-PT/docs/Web/API/BaseAudioContext/destination" title="The documentation about this has not yet been written; please consider contributing!"><code>BaseAudioContext.destination</code></a>, which handles the situation for you:</p>

<pre class="brush: js">track.connect(audioCtx.destination);
</pre>

<p>A good way to visualise these nodes is by drawing an audio graph so you can visualize it. This is what our current audio graph looks like:</p>

<p><img src="https://mdn.mozillademos.org/files/16195/graph1.jpg" alt="an audio graph with an audio element source connected to the default destination" style="border-style: solid; border-width: 1px; height: 486px; width: 1426px;"></p>

<p>Now we can add the play and pause functionality.</p>

<pre class="brush: js">// select our play button
const playButton = document.querySelector(&apos;button&apos;);

playButton.addEventListener(&apos;click&apos;, function() {

    // check if context is in suspended state (autoplay policy)
    if (audioCtx.state === &apos;suspended&apos;) {
        audioCtx.resume();
    }

    // play or pause track depending on state
    if (this.dataset.playing === &apos;false&apos;) {
        audioElement.play();
        this.dataset.playing = &apos;true&apos;;
    } else if (this.dataset.playing === &apos;true&apos;) {
        audioElement.pause();
        this.dataset.playing = &apos;false&apos;;
    }

}, false);
</pre>

<p>We also need to take into account what to do when the track finishes playing. Our <code>HTMLMediaElement</code> fires an <code>ended</code> event once it&apos;s finished playing, so we can listen for that and run code accordingly:</p>

<pre class="brush: js">audioElement.addEventListener(&apos;ended&apos;, () =&gt; {
    playButton.dataset.playing = &apos;false&apos;;
}, false);
</pre>

<h2 id="Um_aparte_sobre_o_editor_de_Áudio_da_Web">Um aparte sobre o editor de Áudio da Web</h2>

<p>Firefox has a tool available called the <a href="/en-US/docs/Tools/Web_Audio_Editor">Web Audio editor</a>. On any page that has an audio graph running on it, you can open the developer tools, and use the Web Audio tab to view the audio graph, see what properties each node has available, and change the values of those properties to see what effect that has.</p>

<p><img src="https://mdn.mozillademos.org/files/16198/web-audio-editor.png" alt="The Firefox web audio editor showing an audio graph with AudioBufferSource, IIRFilter, and AudioDestination" style="border-style: solid; border-width: 1px; height: 365px; width: 1200px;"></p>

<div class="note notecard">
<p><strong>Nota</strong>: The Web Audio editor is not enabled by default. To display it, you need to go into the Firefox developer tools settings and check the <em>Web Audio</em> checkbox in the <em>Default Developer Tools</em> section.</p>
</div>

<h2 id="Modificar_o_som">Modificar o som</h2>

<p>Let&apos;s delve into some basic modification nodes, to change the sound that we have. This is where the Web Audio API really starts to come in handy. First of all, let&apos;s change the volume. This can be done using a <a href="/pt-PT/docs/Web/API/GainNode" title="The documentation about this has not yet been written; please consider contributing!"><code>GainNode</code></a>, which represents how big our sound wave is.</p>

<p>There are two ways you can create nodes with the Web Audio API. You can use the factory method on the context itself (e.g. <code>audioCtx.createGain()</code>) or via a constructor of the node (e.g. <code>new GainNode()</code>). We&apos;ll use the factory method in our code:</p>

<pre class="brush: js">const gainNode = audioCtx.createGain();
</pre>

<p>Now we have to update our audio graph from before, so the input is connected to the gain, then the gain node is connected to the destination:</p>

<pre class="brush: js">track.connect(gainNode).connect(audioCtx.destination);
</pre>

<p>This will make our audio graph look like this:</p>

<p><img src="https://mdn.mozillademos.org/files/16196/graph2.jpg" alt="an audio graph with an audio element source, connected to a gain node that modifies the audio source, and then going to the default destination" style="border-style: solid; border-width: 1px; height: 550px; width: 1774px;"></p>

<p>The default value for gain is 1; this keeps the current volume the same. Gain can be set to a minimum of about -3.4 and a max of about 3.4. Here we&apos;ll allow the boombox to move the gain up to 2 (double the original volume) and down to 0 (this will effectively mute our sound).</p>

<p>Let&apos;s give the user control to do this — we&apos;ll use a <a href="/en-US/docs/Web/HTML/Element/input/range">range input</a>:</p>

<pre class="brush: html">&lt;input type=&quot;range&quot; id=&quot;volume&quot; min=&quot;0&quot; max=&quot;2&quot; value=&quot;1&quot; step=&quot;0.01&quot; /&gt;
</pre>

<div class="note notecard">
<p><strong>Nota</strong>: Range inputs are a really handy input type for updating values on audio nodes. You can specify a range&apos;s values and use them directly with the audio node&apos;s parameters.</p>
</div>

<p>So let&apos;s grab this input&apos;s value and update the gain value when the input node has its value changed by the user:</p>

<pre class="brush: js">const volumeControl = document.querySelector(&apos;#volume&apos;);

volumeControl.addEventListener(&apos;input&apos;, function() {
    gainNode.gain.value = this.value;
}, false);
</pre>

<div class="note notecard">
<p><strong>Nota</strong>: The values of node objects (e.g. <code>GainNode.gain</code>) are not simple values; they are actually objects of type <a href="/pt-PT/docs/Web/API/AudioParam" title="The documentation about this has not yet been written; please consider contributing!"><code>AudioParam</code></a> — these called parameters. This is why we have to set <code>GainNode.gain</code>&apos;s <code>value</code> property, rather than just setting the value on <code>gain</code> directly. This enables them to be much more flexible, allowing for passing the parameter a specific set of values to change between over a set period of time, for example.</p>
</div>

<p>Great, now the user can update the track&apos;s volume! The gain node is the perfect node to use if you want to add mute functionality.</p>

<h2 id="Adicionar_panning_estéreo_à_sua_aplicação">Adicionar <em>panning</em> estéreo à sua aplicação</h2>

<p>Let&apos;s add another modification node to practise what we&apos;ve just learnt.</p>

<p>There&apos;s a <a href="/pt-PT/docs/Web/API/StereoPannerNode" title="The documentation about this has not yet been written; please consider contributing!"><code>StereoPannerNode</code></a> node, which changes the balance of the sound between the left and right speakers, if the user has stereo capabilities.</p>

<p>Note: The <code>StereoPannerNode</code> is for simple cases in which you just want stereo panning from left to right. There is also a <a href="/pt-PT/docs/Web/API/PannerNode" title="The documentation about this has not yet been written; please consider contributing!"><code>PannerNode</code></a>, which allows for a great deal of control over 3D space, or sound <em>spatialisation</em>, for creating more complex effects. This is used in games and 3D apps to create birds flying overhead, or sound coming from behind the user for instance.</p>

<p>To visualise it, we will be making our audio graph look like this:</p>

<p><img src="https://mdn.mozillademos.org/files/16229/graphPan.jpg" alt="An image showing the audio graph showing an input node, two modification nodes (a gain node and a stereo panner node) and a destination node." style="border-style: solid; border-width: 1px; height: 532px; width: 2236px;"></p>

<p>Let&apos;s use the constructor method of creating a node this time. When we do it this way, we have to pass in the context and any options that that particular node may take:</p>

<pre class="brush: js">const pannerOptions = {pan: 0};
const panner = new StereoPannerNode(audioCtx, pannerOptions);
</pre>

<div class="note notecard">
<p><strong>Nota</strong>: The constructor method of creating nodes is not supported by all browsers at this time. The older factory methods are supported more widely.</p>
</div>

<p>Here our values range from -1 (far left) and 1 (far right). Again let&apos;s use a range type input to vary this parameter:</p>

<pre class="brush: html">&lt;input type=&quot;range&quot; id=&quot;panner&quot; min=&quot;-1&quot; max=&quot;1&quot; value=&quot;0&quot; step=&quot;0.01&quot; /&gt;
</pre>

<p>We use the values from that input to adjust our panner values in the same way as we did before:</p>

<pre class="brush: js">const pannerControl = document.querySelector(&apos;#panner&apos;);

pannerControl.addEventListener(&apos;input&apos;, function() {
    panner.pan.value = this.value;
}, false);
</pre>

<p>Let&apos;s adjust our audio graph again, to connect all the nodes together:</p>

<pre class="brush: js">track.connect(gainNode).connect(panner).connect(audioCtx.destination);
</pre>

<p>The only thing left to do is give the app a try: <a href="https://codepen.io/Rumyra/pen/qyMzqN/">Check out the final demo here on Codepen</a>.</p>

<h2 id="Resumo">Resumo</h2>

<p>Great! We have a boombox that plays our &apos;tape&apos;, and we can adjust the volume and stereo panning, giving us a fairly basic working audio graph.</p>

<p>This makes up quite a few basics that you would need to start to add audio to your website or web app. There&apos;s a lot more functionality to the Web Audio API, but once you&apos;ve grasped the concept of nodes and putting your audio graph together, we can move on to looking at more complex functionality.</p>

<h2 id="Mais_exemplos">Mais exemplos</h2>

<p>There are other examples available to learn more about the Web Audio API.</p>

<p>The <a href="https://github.com/mdn/voice-change-o-matic">Voice-change-O-matic</a> is a fun voice manipulator and sound visualization web app that allows you to choose different effects and visualizations. The application is fairly rudimentary, but it demonstrates the simultaneous use of multiple Web Audio API features. (<a href="https://mdn.github.io/voice-change-o-matic/">run the Voice-change-O-matic live</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7921/voice-change-o-matic.png" alt="A UI with a sound wave being shown, and options for choosing voice effects and visualizations." style="border-style: solid; border-width: 1px; display: block; height: 500px; margin: 0px auto; width: 640px;"></p>

<p>Another application developed specifically to demonstrate the Web Audio API is the <a href="http://mdn.github.io/violent-theremin/">Violent Theremin</a>, a simple web application that allows you to change pitch and volume by moving your mouse pointer. It also provides a psychedelic lightshow (<a href="https://github.com/mdn/violent-theremin">see Violent Theremin source code</a>).</p>

<p><img src="https://mdn.mozillademos.org/files/7919/violent-theremin.png" alt="A page full of rainbow colours, with two buttons labeled Clear screen and mute. " style="border-style: solid; border-width: 1px; display: block; height: 458px; margin: 0px auto; width: 640px;"></p>

<p>Consulte também a nossa <a href="https://github.com/mdn/webaudio-examples">repositório de exemplos de áudio</a> para mais exemplos.</p>
