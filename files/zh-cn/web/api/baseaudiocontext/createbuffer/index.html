---
title: AudioContext.createBuffer()
slug: Web/API/BaseAudioContext/createBuffer
tags:
  - 创建音频片段
  - 接口
  - 方法
  - 音频环境
translation_of: Web/API/BaseAudioContext/createBuffer
original_slug: Web/API/AudioContext/createBuffer
---
<p>音频环境<a href="/zh-CN/docs/Web/API/AudioContext" title="AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。"><code>AudioContext</code></a> 接口的 <code>createBuffer() 方法用于新建一个空</code>白的 <a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a> 对象，以便用于填充数据，通过 <a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a> 播放。</p>

<p>更多关于音频片段(Audio Buffer)的细节，请参考<a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a>页面。</p>

<div class="note notecard">
<p><strong>注意：</strong> <code>createBuffer()</code> 曾被用于接收压缩后的音频数据，并返回被解码的音频，但是这项功能现在已经被移除，因为所有的解码工作应当在主线程中被完成，<code>createBuffer()</code> 阻塞了其他代码的执行。异步方法 <code>decodeAudioData()</code> 能够完成相同的工作 —— 传入一个压缩过的音频（如MP3格式的文件），并直接返回一个可以通过 <a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a> 播放的 <a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a> 。因此播放诸如MP3等格式的压缩音频时，你应当使用 <code>decodeAudioData() 方法。</code></p>
</div>

<h2 id="语法">语法</h2>

<pre>AudioContext.createBuffer(Number numOfChannels, Number length, Number sampleRate);</pre>

<h3 id="参数">参数</h3>

<div class="note notecard">
<p><strong>注意：</strong>如果想深入了解 audio buffers 是如何工作的、这些参数的具体含义，请阅读这篇简短的指南： <a href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API#Audio_buffers.3A_frames.2C_samples_and_channels">Audio buffers: frames, samples and channels</a>（英）。</p>
</div>

<dl>
 <dt>numOfChannels</dt>
 <dd>一个定义了 buffer 中包含的声频通道数量的整数。<br>
 一个标准的实现必须包含至少32个声频通道。</dd>
 <dt> </dt>
 <dt>length</dt>
 <dd>一个代表 buffer 中的样本帧数的整数。</dd>
 <dt>sampleRate</dt>
 <dd>线性音频样本的采样率，即每一秒包含的关键帧的个数。实现过程中必须支持 22050～96000的采样率。</dd>
</dl>

<p> </p>

<h3 id="返回值">返回值</h3>

<p>一个 <a href="/zh-CN/docs/Web/API/AudioBuffer" title="这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。"><code>AudioBuffer</code></a>。</p>

<h2 id="示例">示例</h2>

<p>首先，我们将从几个浅显易懂的示例入手，来解释如何使用这些参数：</p>

<pre class="brush: js">var audioCtx = new AudioContext();
var buffer = audioCtx.createBuffer(2, 22050, 44100);</pre>

<p>如果你这样调用，你将会得到一个立体声（两个声道）的音频片段(Buffer)，当它在一个频率为44100赫兹（这是目前大部分声卡处理声音的频率）的音频环境(<a href="/zh-CN/docs/Web/API/AudioContext" title="AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。"><code>AudioContext</code></a>)中播放的时候，会持续0.5秒：22050帧 / 44100赫兹 = 0.5 秒。</p>

<pre class="brush: js">var audioCtx = new AudioContext();
var buffer = audioCtx.createBuffer(1, 22050, 22050);</pre>

<p>如果你这样调用，你将会得到一个单声道的音频片段(Buffer)，当它在一个频率为44100赫兹的音频环境(<a href="/zh-CN/docs/Web/API/AudioContext" title="AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。"><code>AudioContext</code></a>)中播放的时候，将会被自动按照44100赫兹*重采样*（因此也会转化为44100赫兹的片段），并持续1秒：44100帧 / 44100赫兹 = 1秒。</p>

<div class="note notecard">
<p><strong>注意：</strong> <font face="Consolas, Liberation Mono, Courier, monospace">音频重采样与图片的缩放非常类似：比如你有一个</font>16 x 16的图像，但是你想把它填充到一个32 x 32大小的区域，你就要对它进行缩放（重采样）。得到的结果会是一个叫低品质的（图像会模糊或者有锯齿形的边缘，这取决于缩放采用的算法），但它却是能将原图形缩放，并且缩放后的图像占用空间比相同大小的普通图像要小。重新采样的音频道理相同——你会介于一些空间，但事实上你无法产出高频率的声音（高音区）。</p>
</div>

<p>现在让我们来看一个更加复杂的示例，我们将创建一个时长2秒的音频片段，并用白噪声填充它，之后通过一个 音频片段源节点(<a href="/zh-CN/docs/Web/API/AudioBufferSourceNode" title="AudioBufferSourceNode 接口继承自,表现为一个音频源，它包含了一些写在内存中的音频数据，通常储存在一个ArrayBuffer对象中。在处理有严格的时间精确度要求的回放的情形下它尤其有用。比如播放那些需要满足一个指定节奏的声音或者那些储存在内存而不是硬盘或者来自网络的声音。为了播放那些有时间精确度需求但来自网络的流文件或者来自硬盘，则使用"><code>AudioBufferSourceNode</code></a>) 播放。代码中的注释应该能充分解释发生了什么。你可以 <a href="http://mdn.github.io/audio-buffer/">在线演示</a> ，或者 <a href="https://github.com/mdn/audio-buffer">查看源代码</a> 。</p>

<pre class="brush: js;highlight[13]">var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
var button = document.querySelector(&apos;button&apos;);
var pre = document.querySelector(&apos;pre&apos;);
var myScript = document.querySelector(&apos;script&apos;);

pre.innerHTML = myScript.innerHTML;

// 立体声
var channels = 2;
// 创建一个 采样率与音频环境(AudioContext)相同的 时长2秒的 音频片段。
var frameCount = audioCtx.sampleRate * 2.0;

var myArrayBuffer = audioCtx.createBuffer(channels, frameCount, audioCtx.sampleRate);

button.onclick = function() {
  // 使用白噪声填充;
  // 就是 -1.0 到 1.0 之间的随机数
  for (var channel = 0; channel &lt; channels; channel++) {
   // 这允许我们读取实际音频片段(AudioBuffer)中包含的数据
   var nowBuffering = myArrayBuffer.getChannelData(channel);
   for (var i = 0; i &lt; frameCount; i++) {
     // Math.random() is in [0; 1.0]
     // audio needs to be in [-1.0; 1.0]
     nowBuffering[i] = Math.random() * 2 - 1;
   }
  }

  // 获取一个 音频片段源节点(AudioBufferSourceNode)。
  // 当我们想播放音频片段时，我们会用到这个源节点。
  var source = audioCtx.createBufferSource();
  // 把刚才生成的片段加入到 音频片段源节点(AudioBufferSourceNode)。
  source.buffer = myArrayBuffer;
  // 把 音频片段源节点(AudioBufferSourceNode) 连接到
  // 音频环境(AudioContext) 的终节点，这样我们就能听到声音了。
  source.connect(audioCtx.destination);
  // 开始播放声源
  source.start();
}</pre>

<h2 id="规范">规范</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">规范</th>
   <th scope="col">现状</th>
   <th scope="col">备注</th>
  </tr>
  <tr>
   <td><a lang="en" href="https://webaudio.github.io/web-audio-api/#widl-AudioContext-createBuffer-AudioBuffer-unsigned-long-numberOfChannels-unsigned-long-length-float-sampleRate" class="external" hreflang="en">Web Audio API<br><small lang="zh-CN">createBuffer()</small></a></td>
   <td><span class="spec-WD">Working Draft</span></td>
   <td> </td>
  </tr>
 </tbody>
</table>

<h2 id="浏览器兼容性">浏览器兼容性</h2>

<div><div class="warning notecard"><strong><a href="https://github.com/mdn/browser-compat-data">We&apos;re converting our compatibility data into a machine-readable JSON format</a></strong>.
            This compatibility table still uses the old format,
            because we haven&apos;t yet converted the data it contains.
            <strong><a href="/zh-CN/docs/MDN/Contribute/Structures/Compatibility_tables">Find out how you can help!</a></strong></div>

<div class="htab">
    <a id="AutoCompatibilityTable" name="AutoCompatibilityTable"></a>
    <ul>
        <li class="selected"><a>Desktop</a></li>
        <li><a>Mobile</a></li>
    </ul>
</div></div>

<div id="compat-desktop">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Chrome</th>
   <th>Firefox (Gecko)</th>
   <th>Internet Explorer</th>
   <th>Opera</th>
   <th>Safari (WebKit)</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td>10.0<span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes">webkit</a></span></td>
   <td><a href="/en-US/Firefox/Releases/25">25.0</a> (25.0) </td>
   <td><span style="color: #f00;">未实现</span></td>
   <td>15.0 <span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes">webkit</a></span><br>
    22</td>
   <td>6.0<span class="prefixBox prefixBoxInline notecard inline" title="prefix"><a href="/zh-CN/docs/Web/Guide/Prefixes">webkit</a></span></td>
  </tr>
 </tbody>
</table>
</div>

<div id="compat-mobile">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Android</th>
   <th>Firefox Mobile (Gecko)</th>
   <th>Firefox OS</th>
   <th>IE Mobile</th>
   <th>Opera Mobile</th>
   <th>Safari Mobile</th>
   <th>Chrome for Android</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td><span style="color: rgb(255, 153, 0);" title="Compatibility unknown; please update this.">?</span></td>
   <td>26.0</td>
   <td>1.2</td>
   <td><span style="color: rgb(255, 153, 0);" title="Compatibility unknown; please update this.">?</span></td>
   <td><span style="color: rgb(255, 153, 0);" title="Compatibility unknown; please update this.">?</span></td>
   <td><span style="color: rgb(255, 153, 0);" title="Compatibility unknown; please update this.">?</span></td>
   <td>33.0</td>
  </tr>
 </tbody>
</table>
</div>

<h2 id="相关链接">相关链接</h2>

<ul>
 <li><a href="/en-US/docs/Web_Audio_API/Using_Web_Audio_API">使用网络音频接口（英文）</a></li>
</ul>
